---
title: "R Notebook"
output: html_notebook
---

---
title: "APM chapter 12.3 excercise"
output: html_notebook
author: "Haroun Simbiat, Okechukwu Princewill, Olatunde Olanayo"
date: "17/02/2020"
---

```{r}
library(caret)
library(klaR)
library(corrplot)
library(plyr)
library(dplyr)
library(pROC)
library(party)
library(stringr)
```


## Question 12.3a
### (a) Explore the data by visualizing the relationship between the predictors and the outcome. Are there important features of the predictor data themselves, such as between-predictor correlations or degenerate distributions? Can functions of more than one predictor be used to model the data more effectively?
```{r}
# load the CSV file from the local directory
test <- read.csv("churnTest.csv")
train <- read.csv("churnTrain.csv")
```

## Descriptive Exploration 
```{r}
# dimension of dataset
dim(train)
dim(test)
```

```{r}
head(train)
View(train)
```

```{r}
str(train)
```


```{r}
# Remove redundant variable x and state
dataset <- train[,-c(1,2)]
```

```{r}
# Remove "area_code_" from column area_code
dataset$area_code <- as.factor(str_remove_all(dataset$area_code, "[area_code_]"))
```

```{r}
# Change the values in the mentioned columns  from “no” or “yes” to 0 or 1
dataset$international_plan <- as.factor(mapvalues(dataset$international_plan, from=c("no", "yes"), to=c("0","1")))
dataset$voice_mail_plan <- mapvalues(dataset$voice_mail_plan, from=c("no", "yes"), to=c("0","1"))
```

Let’s go ahead and convert the dataset to numeric values.
```{r}
# convert input values to numeric
for(i in 1:18) {
dataset[,i] <- as.numeric(as.character(dataset[,i]))
}
```

Now, let’s take a summary of the data and see what we have.
```{r}
summary(dataset)
```
Interestingly, We can also see that there is some imbalance in the Class values. Let’s take a closer look at the breakdown of the Class values. We can also see that all attributes have integer values of different range, this suggests that we may see much benefit from normalizing attributes.

```{r}
# class distribution
cbind(freq=table(dataset$churn), percentage=prop.table(table(dataset$churn))*100)
plot(dataset$churn)
```
There is indeed a 86% to 13% split for no-yes in the class values which is imbalanced, we need to be thinking about rebalancing the dataset, at least not yet.
```{r}
View(cor(dataset[,1:18]))
```
We can see some modest to high correlation between some of the attributes. For example between total_day_charge and total_day_minutes at 0.99 correlation. Some algorithms may benefit from removing the highly correlated attributes.



## Now, let’s take a look at the interactions between the attributes. Let’s start with a scatter plot matrix of the attributes colored by the class values. Because the data is discrete (integer values) we need to add some jitter to make the scatter plots useful, otherwise the dots will all be on top of each other.
```{r}
# scatter plot matrix
jittered_x <- sapply(dataset[,1:18], jitter)
pairs(jittered_x, names(dataset[,1:18]), col=dataset$churn)
```
We can see that the red (yes) the (smaller values) and black (no) are all over the place.
```{r}
# bar plots of each variable by class
par(mfrow=c(3,2))
for(i in 1:18) {
barplot(table(dataset$churn,dataset[,i]), main=names(dataset)[i],
legend.text=unique(dataset$churn))
}
```

```{r}
# Check out out for zero variance columns and remove it 
zero_col <- nearZeroVar(dataset)
print(names(dataset)[zero_col])
dataset = dataset[,-zero_col]
```

```{r}
# Correlation Plot
corr_plot <- cor(dataset[,1:17])
corrplot(corr_plot, method = "circle")
```
The plot above confirmed the corrolation we got from the descriptive statisitics, we will go on to remove them from the dataset.

```{r}
# Find and remove highly correlated attributes

cutoff <- 0.99 
correlations <- cor(dataset[,1:17])
highlyCorrelated <- findCorrelation(correlations, cutoff=cutoff)

for (value in highlyCorrelated) {
print(names(dataset)[value])
}

# create a new dataset without highly correlated features
dataset <- dataset[,-highlyCorrelated]
dim(dataset)
```
you can see that the 4 correlated predictors has been removed and the dimension of he dataset has also been reduced.
  
  
## Question 12.3b
### Fit some basic models to the training set and tune them via resampling. What criteria should be used to evaluate the effectiveness of the models?

When comapring multiple  prediction from imbalanced classificain problems, consider using metrics metrics beyond accuracy such as Recall, Precision, Lift and Area uner ROC curve wil be instrumental in determing which is superior to the others.
```{r}
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)
metric <- "ROC"

# LG
set.seed(7)
fit.glm <- train(churn~., data=dataset, method="glm", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# LDA
set.seed(7)
fit.lda <- train(churn~., data=dataset, method="lda", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(churn~., data=dataset, method="glmnet", metric=metric,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# KNN
set.seed(7)
fit.knn <- train(churn~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# CART
set.seed(7)
fit.cart <- train(churn~., data=dataset, method="rpart", metric=metric,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# Naive Bayes
set.seed(7)
fit.nb <- train(churn~., data=dataset, method="nb", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# SVM
set.seed(7)
fit.svm <- train(churn~., data=dataset, method="svmRadial", metric=metric,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

# Compare algorithms
transformResults <- resamples(list(LG=fit.glm, LDA=fit.lda, GLMNET=fit.glmnet, KNN=fit.knn, CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(transformResults)
dotplot(transformResults)
```


### Tuning SVM
The SVM implementation has two parameters that we can tune with the caret package: sigma
which is a smoothing term and C which is a cost constraint. Let’s try a range of values for C between
1 and 10 and a few small values for sigma around the default of 0.1.
```{r}
# 10-fold cross-validation with 3 repeats

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)
metric <- "ROC"

set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
fit.svm <- train(churn~., data=dataset, method="svmRadial", metric=metric, tuneGrid=grid, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
print(fit.svm)

plot(fit.svm)
```
We can see that we have made very little difference to the results. the most accurate model had a score of 90.54% for ROC (the same as our precioiusly rounded score of 0.90), slightly improve on the sensitivity with 98.10% and also drop in the value of specificity of 50.29% using sigma = 0.025 and c = 5.

### Tuning CART
The CART implementation has one paremeter that will can tue with caret: complexity parameter(cp). Let's try the comcination of 0, 0.05 and 0.01 on cp.
```{r}
# 10-fold cross-validation with 3 repeats

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)
metric <- "ROC"

set.seed(7)
grid <- expand.grid(.cp = c(0, 0.05, 0.01))
fit.cart <- train(churn~., data=dataset, method="rpart", metric=metric, tuneGrid=grid, preProc=c("center", "scale", "BoxCox"), trControl=trainControl)

print(fit.cart)
plot(fit.cart)
```
We can see here that tunning has a made a significance differnce, setting on a value of cp = 0 a ROC of 89.79% was achieved, sensitivity of 98.03%, and specitivity of 71.38%. This outcome is higher than the previous model. 

### Finalize Model
We now need to finalize the mode, which really means to chooose which model we would like to use. for simplicity and performance i would select CART method. We will have to prepare the testing dataset for the preprocessing like the way we did to training dataset.
```{r}
# Remove redundant variable x and state from the testing dataset
validation <- test[,-c(1,2)]

# Remove "area_code_" from column area_code from the testing dataset
validation$area_code <- as.factor(str_remove_all(validation$area_code, "[area_code_]"))

# Change the values in the mentioned columns  from “no” or “yes” to 0 or 1 from the testing dataset
validation$international_plan <- as.factor(mapvalues(validation$international_plan, from=c("no", "yes"), to=c("0","1")))

validation$voice_mail_plan <- mapvalues(validation$voice_mail_plan, from=c("no", "yes"), to=c("0","1"))

# convert input values to numeric from the testing dataset
for(i in 1:18) {
validation[,i] <- as.numeric(as.character(validation[,i]))
}

# Check out out for zero variance columns and remove it from the validation dataset
val_zero_col <- nearZeroVar(validation)
print(names(validation)[val_zero_col])
validation = validation[,-val_zero_col]

# Find and remove highly correlated attributes
val_cutoff <- 0.99 
val_correlations <- cor(validation[,1:17])
val_highlyCorrelated <- findCorrelation(val_correlations, cutoff= val_cutoff)

for (value in val_highlyCorrelated) {
print(names(validation)[value])
}

# create a new dataset without highly correlated features
validation <- validation[,-val_highlyCorrelated]
dim(validation)

```

Data transforming
```{r}
set.seed(7)
# transform the training dataset
preProcess <- preProcess(dataset, method = c("center", "scale", "BoxCox"))
x_prep <- predict(preProcess, dataset)
x <- x_prep[,1:13]
y <- x_prep[,14]

set.seed(7)
# transform the validation dataset
X_prep <- predict(preProcess, validation)
X <- X_prep[,1:13]
Y <- X_prep[,14]
```



Now that we know a good algorithm (Random Forest) and the good configuration (cp=0, minsplit=2, minbucket=1) we can create the final model directly using all  of the training data.
```{r}
# make predictions
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

# Fit training dataset
set.seed(7)
finalModel <- rpart(churn~., data=x_prep, cp=0.0089286, method = "class", minsplit = 2, minbucket = 1)

# Predict training set
finalPridiction_x <- predict(finalModel, x, type = "class") 
confusionMatrix(finalPridiction_x, y)

finalPridiction <- predict(finalModel, X, type = "class") 
confusionMatrix(finalPridiction, Y)

fancyRpartPlot(finalModel, caption = NULL)

finalModel$variable.importance

printcp(finalModel)

mytree <- prune(finalModel, cp = 0.0089286)

```


```{r}
finalPridiction_Prob <- predict(finalModel, X) 
rocCurve <- roc(response = Y, predictor = finalPridiction_Prob[,"no"], levels = rev(levels(Y)))
auc(rocCurve)

plot(rocCurve, legacy.axes=T, add=F, col="black")
```


## Question 12.3c
### Use lift charts to compare models. If you wanted to identify 80 % of the churning customers, how many other customers would also be identified?
```{r}
metric2 <- validation
prob<- predict(finalModel, metric2[,-14]) 
metric2$probaNO <- prob[,"no"]
metric2$probaYES <- prob[,"yes"]

liftcurve <- lift(churn ~ probaNO + probaYES, data = metric2, labels = c(probaNO="CART", probaYES="CARTYES"))
xyplot(liftcurve, auto.key = list(columns = 2, lines = TRUE, points = FALSE))
```


